{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a quick first pass for the all-state kaggle competition.  No data transforms or explaration, just a quick and dirty  neural net.\n",
    "\n",
    "Run time is about 10 mins.   Development time ~ 1 hour\n",
    "Hardware GeForce GTX 650 Ti\n",
    "Intel(R) Core(TM) i5-2500K CPU @ 3.30GHz\n",
    "DIMM DDR3 Synchronous 1333 MHz (0.8 ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"train.csv\")\n",
    "test=pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718367</td>\n",
       "      <td>0.335060</td>\n",
       "      <td>0.30260</td>\n",
       "      <td>0.67135</td>\n",
       "      <td>0.83510</td>\n",
       "      <td>0.569745</td>\n",
       "      <td>0.594646</td>\n",
       "      <td>0.822493</td>\n",
       "      <td>0.714843</td>\n",
       "      <td>2213.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438917</td>\n",
       "      <td>0.436585</td>\n",
       "      <td>0.60087</td>\n",
       "      <td>0.35127</td>\n",
       "      <td>0.43919</td>\n",
       "      <td>0.338312</td>\n",
       "      <td>0.366307</td>\n",
       "      <td>0.611431</td>\n",
       "      <td>0.304496</td>\n",
       "      <td>1283.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289648</td>\n",
       "      <td>0.315545</td>\n",
       "      <td>0.27320</td>\n",
       "      <td>0.26076</td>\n",
       "      <td>0.32446</td>\n",
       "      <td>0.381398</td>\n",
       "      <td>0.373424</td>\n",
       "      <td>0.195709</td>\n",
       "      <td>0.774425</td>\n",
       "      <td>3005.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440945</td>\n",
       "      <td>0.391128</td>\n",
       "      <td>0.31796</td>\n",
       "      <td>0.32128</td>\n",
       "      <td>0.44467</td>\n",
       "      <td>0.327915</td>\n",
       "      <td>0.321570</td>\n",
       "      <td>0.605077</td>\n",
       "      <td>0.602642</td>\n",
       "      <td>939.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178193</td>\n",
       "      <td>0.247408</td>\n",
       "      <td>0.24564</td>\n",
       "      <td>0.22089</td>\n",
       "      <td>0.21230</td>\n",
       "      <td>0.204687</td>\n",
       "      <td>0.202213</td>\n",
       "      <td>0.246011</td>\n",
       "      <td>0.432606</td>\n",
       "      <td>2763.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9   ...        cont6  \\\n",
       "0   1    A    B    A    B    A    A    A    A    B   ...     0.718367   \n",
       "1   2    A    B    A    A    A    A    A    A    B   ...     0.438917   \n",
       "2   5    A    B    A    A    B    A    A    A    B   ...     0.289648   \n",
       "3  10    B    B    A    B    A    A    A    A    B   ...     0.440945   \n",
       "4  11    A    B    A    B    A    A    A    A    B   ...     0.178193   \n",
       "\n",
       "      cont7    cont8    cont9   cont10    cont11    cont12    cont13  \\\n",
       "0  0.335060  0.30260  0.67135  0.83510  0.569745  0.594646  0.822493   \n",
       "1  0.436585  0.60087  0.35127  0.43919  0.338312  0.366307  0.611431   \n",
       "2  0.315545  0.27320  0.26076  0.32446  0.381398  0.373424  0.195709   \n",
       "3  0.391128  0.31796  0.32128  0.44467  0.327915  0.321570  0.605077   \n",
       "4  0.247408  0.24564  0.22089  0.21230  0.204687  0.202213  0.246011   \n",
       "\n",
       "     cont14     loss  \n",
       "0  0.714843  2213.18  \n",
       "1  0.304496  1283.60  \n",
       "2  0.774425  3005.09  \n",
       "3  0.602642   939.85  \n",
       "4  0.432606  2763.85  \n",
       "\n",
       "[5 rows x 132 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have categorical and continuous variables.  Lets take a look at the size of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188318, 132)\n",
      "(125546, 131)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just in case I want to combine the two sets, I will add a 'loss' column to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['loss']=np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am just going to select out the feature that are categorical and those that are continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat_cont = [f for f in train.columns if 'cont' in f]\n",
    "feat_cat = [f for f in train.columns if 'cat' in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the range on continuous variables--first put all the data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cont1 0.984975 1.6e-05\n",
      "cont2 0.862654 0.001149\n",
      "cont3 0.944251 0.002634\n",
      "cont4 0.956046 0.176921\n",
      "cont5 0.983674 0.281143\n",
      "cont6 0.997162 0.012683\n",
      "cont7 1.0 0.069503\n",
      "cont8 0.9828 0.23688\n",
      "cont9 0.9954 8e-05\n",
      "cont10 0.99498 0.0\n",
      "cont11 0.998742 0.035321\n",
      "cont12 0.998484 0.036232\n",
      "cont13 0.988494 0.000228\n",
      "cont14 0.844848 0.178568\n"
     ]
    }
   ],
   "source": [
    "data=train\n",
    "n=train.shape[0]\n",
    "data=data.append(test)\n",
    "for col in feat_cont:\n",
    "    print(col, data[col].max(),data[col].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like they are mostly between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change each label to a numeric label.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le=preprocessing.LabelEncoder()\n",
    "for col in feat_cat:\n",
    "    data[col]=le.fit_transform(data[col])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat1 1 0\n",
      "cat2 1 0\n",
      "cat3 1 0\n",
      "cat4 1 0\n",
      "cat5 1 0\n",
      "cat6 1 0\n",
      "cat7 1 0\n",
      "cat8 1 0\n",
      "cat9 1 0\n",
      "cat10 1 0\n",
      "cat11 1 0\n",
      "cat12 1 0\n",
      "cat13 1 0\n",
      "cat14 1 0\n",
      "cat15 1 0\n",
      "cat16 1 0\n",
      "cat17 1 0\n",
      "cat18 1 0\n",
      "cat19 1 0\n",
      "cat20 1 0\n",
      "cat21 1 0\n",
      "cat22 1 0\n",
      "cat23 1 0\n",
      "cat24 1 0\n",
      "cat25 1 0\n",
      "cat26 1 0\n",
      "cat27 1 0\n",
      "cat28 1 0\n",
      "cat29 1 0\n",
      "cat30 1 0\n",
      "cat31 1 0\n",
      "cat32 1 0\n",
      "cat33 1 0\n",
      "cat34 1 0\n",
      "cat35 1 0\n",
      "cat36 1 0\n",
      "cat37 1 0\n",
      "cat38 1 0\n",
      "cat39 1 0\n",
      "cat40 1 0\n",
      "cat41 1 0\n",
      "cat42 1 0\n",
      "cat43 1 0\n",
      "cat44 1 0\n",
      "cat45 1 0\n",
      "cat46 1 0\n",
      "cat47 1 0\n",
      "cat48 1 0\n",
      "cat49 1 0\n",
      "cat50 1 0\n",
      "cat51 1 0\n",
      "cat52 1 0\n",
      "cat53 1 0\n",
      "cat54 1 0\n",
      "cat55 1 0\n",
      "cat56 1 0\n",
      "cat57 1 0\n",
      "cat58 1 0\n",
      "cat59 1 0\n",
      "cat60 1 0\n",
      "cat61 1 0\n",
      "cat62 1 0\n",
      "cat63 1 0\n",
      "cat64 1 0\n",
      "cat65 1 0\n",
      "cat66 1 0\n",
      "cat67 1 0\n",
      "cat68 1 0\n",
      "cat69 1 0\n",
      "cat70 1 0\n",
      "cat71 1 0\n",
      "cat72 1 0\n",
      "cat73 2 0\n",
      "cat74 2 0\n",
      "cat75 2 0\n",
      "cat76 2 0\n",
      "cat77 3 0\n",
      "cat78 3 0\n",
      "cat79 3 0\n",
      "cat80 3 0\n",
      "cat81 3 0\n",
      "cat82 3 0\n",
      "cat83 3 0\n",
      "cat84 3 0\n",
      "cat85 3 0\n",
      "cat86 3 0\n",
      "cat87 3 0\n",
      "cat88 3 0\n",
      "cat89 8 0\n",
      "cat90 6 0\n",
      "cat91 7 0\n",
      "cat92 8 0\n",
      "cat93 4 0\n",
      "cat94 6 0\n",
      "cat95 4 0\n",
      "cat96 8 0\n",
      "cat97 6 0\n",
      "cat98 4 0\n",
      "cat99 16 0\n",
      "cat100 14 0\n",
      "cat101 18 0\n",
      "cat102 8 0\n",
      "cat103 13 0\n",
      "cat104 16 0\n",
      "cat105 19 0\n",
      "cat106 17 0\n",
      "cat107 19 0\n",
      "cat108 10 0\n",
      "cat109 84 0\n",
      "cat110 133 0\n",
      "cat111 16 0\n",
      "cat112 50 0\n",
      "cat113 62 0\n",
      "cat114 18 0\n",
      "cat115 22 0\n",
      "cat116 348 0\n"
     ]
    }
   ],
   "source": [
    "for col in feat_cat:\n",
    "    print(col, data[col].max(),data[col].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like 70 are single variable (1,0), and 45 with multi-columns.  WHile we can one-hot encode, for this trail, we will just stick with the label to numeric conversion.  This is not a safe assumption, but ok for a first pass.\n",
    "\n",
    "Now lets just scale all the variables so they have the same variance and are centered around 0.\n",
    "First I will drop off the loss column though "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss=data['loss']\n",
    "data.drop(['loss'],inplace=True,axis=1)\n",
    "data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Now create the training data x_train, the targets 'y' and separate out the test data 'test_input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train=data[0:n,:]\n",
    "test_input=data[n:,:]\n",
    "y=loss[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Neural Net.  400x200x50\n",
    "Using MAE loss as that was the measurement of the competition.\n",
    "Dropouts of 40%, 20%, 20% (Prevents overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "    \n",
    "model.add(Dense(400, input_dim = x_train.shape[1], init = 'he_normal'))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "     \n",
    "model.add(Dense(200, init = 'he_normal'))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())    \n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(50, init = 'he_normal'))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())    \n",
    "model.add(Dropout(0.2))\n",
    "    \n",
    "model.add(Dense(1, init = 'he_normal'))\n",
    "model.compile(loss = 'mae', optimizer = 'adadelta')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 150654 samples, validate on 37664 samples\n",
      "Epoch 1/10\n",
      "150654/150654 [==============================] - 48s - loss: 1739.2924 - val_loss: 1276.8897\n",
      "Epoch 2/10\n",
      "150654/150654 [==============================] - 47s - loss: 1552.4990 - val_loss: 1236.6918\n",
      "Epoch 3/10\n",
      "150654/150654 [==============================] - 46s - loss: 1515.2069 - val_loss: 1212.5791\n",
      "Epoch 4/10\n",
      "150654/150654 [==============================] - 48s - loss: 1489.4862 - val_loss: 1204.1081\n",
      "Epoch 5/10\n",
      "150654/150654 [==============================] - 47s - loss: 1463.0198 - val_loss: 1195.3877\n",
      "Epoch 6/10\n",
      "150654/150654 [==============================] - 47s - loss: 1439.9819 - val_loss: 1193.4363\n",
      "Epoch 7/10\n",
      "150654/150654 [==============================] - 48s - loss: 1422.5955 - val_loss: 1189.2761\n",
      "Epoch 8/10\n",
      "150654/150654 [==============================] - 50s - loss: 1407.2862 - val_loss: 1189.0931\n",
      "Epoch 9/10\n",
      "150654/150654 [==============================] - 50s - loss: 1392.8312 - val_loss: 1183.5734\n",
      "Epoch 10/10\n",
      "150654/150654 [==============================] - 47s - loss: 1370.1335 - val_loss: 1175.4425\n"
     ]
    }
   ],
   "source": [
    "outputs=model.fit(x_train, y, batch_size=32, nb_epoch=10, verbose=1, validation_split=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds=model.predict( test_input, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1567.00280762],\n",
       "       [ 1642.50939941],\n",
       "       [ 9320.90429688],\n",
       "       ..., \n",
       "       [ 2483.25976562],\n",
       "       [ 1282.08630371],\n",
       "       [ 3342.95410156]], dtype=float32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Little quirck on the output--even though preds is a 1-d array, it is a numpy array of length (n,1) which pandas see as a 2-d array.  to get around this, we either flatter or use ravel\n",
    "\n",
    "I could probably just write to file, but this presents a nice view of the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output=pd.DataFrame({'id':test['id'],'loss':preds.ravel()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1567.002808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>1642.509399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>9320.904297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>5998.083008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>882.344055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17</td>\n",
       "      <td>1835.220581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>2105.648926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>28</td>\n",
       "      <td>1097.714600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>32</td>\n",
       "      <td>2825.964355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>43</td>\n",
       "      <td>3117.325439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>46</td>\n",
       "      <td>2651.223389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>50</td>\n",
       "      <td>1157.201416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>54</td>\n",
       "      <td>1314.928711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>62</td>\n",
       "      <td>2376.517822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>70</td>\n",
       "      <td>1742.467407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>71</td>\n",
       "      <td>5445.864746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>75</td>\n",
       "      <td>1979.991455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>77</td>\n",
       "      <td>2537.429688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>81</td>\n",
       "      <td>2123.841797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>83</td>\n",
       "      <td>2320.404053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>87</td>\n",
       "      <td>1807.947388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>97</td>\n",
       "      <td>1847.058105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>103</td>\n",
       "      <td>1144.983398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>119</td>\n",
       "      <td>1103.953491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>120</td>\n",
       "      <td>1414.260254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>127</td>\n",
       "      <td>1052.531860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>138</td>\n",
       "      <td>4996.552246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>141</td>\n",
       "      <td>2968.937744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>148</td>\n",
       "      <td>932.266968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>150</td>\n",
       "      <td>2150.988525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125516</th>\n",
       "      <td>587482</td>\n",
       "      <td>1385.335938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125517</th>\n",
       "      <td>587484</td>\n",
       "      <td>4998.928223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125518</th>\n",
       "      <td>587489</td>\n",
       "      <td>1797.644531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125519</th>\n",
       "      <td>587494</td>\n",
       "      <td>1685.208862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125520</th>\n",
       "      <td>587509</td>\n",
       "      <td>1082.122192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125521</th>\n",
       "      <td>587511</td>\n",
       "      <td>913.553467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125522</th>\n",
       "      <td>587515</td>\n",
       "      <td>1287.169556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125523</th>\n",
       "      <td>587517</td>\n",
       "      <td>2376.877686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125524</th>\n",
       "      <td>587519</td>\n",
       "      <td>1525.345947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125525</th>\n",
       "      <td>587524</td>\n",
       "      <td>859.239380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125526</th>\n",
       "      <td>587531</td>\n",
       "      <td>5675.145020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125527</th>\n",
       "      <td>587532</td>\n",
       "      <td>6627.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125528</th>\n",
       "      <td>587534</td>\n",
       "      <td>1983.960205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125529</th>\n",
       "      <td>587538</td>\n",
       "      <td>2301.583252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125530</th>\n",
       "      <td>587540</td>\n",
       "      <td>3059.468262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125531</th>\n",
       "      <td>587548</td>\n",
       "      <td>1145.381592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125532</th>\n",
       "      <td>587549</td>\n",
       "      <td>7495.793945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125533</th>\n",
       "      <td>587560</td>\n",
       "      <td>3839.278320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125534</th>\n",
       "      <td>587561</td>\n",
       "      <td>1589.709595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125535</th>\n",
       "      <td>587581</td>\n",
       "      <td>1331.937256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125536</th>\n",
       "      <td>587583</td>\n",
       "      <td>1663.251465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125537</th>\n",
       "      <td>587587</td>\n",
       "      <td>1705.305908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125538</th>\n",
       "      <td>587596</td>\n",
       "      <td>1809.606934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125539</th>\n",
       "      <td>587610</td>\n",
       "      <td>1721.611816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125540</th>\n",
       "      <td>587613</td>\n",
       "      <td>1630.627563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125541</th>\n",
       "      <td>587617</td>\n",
       "      <td>2110.746338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125542</th>\n",
       "      <td>587621</td>\n",
       "      <td>2785.067871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125543</th>\n",
       "      <td>587627</td>\n",
       "      <td>2483.259766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125544</th>\n",
       "      <td>587629</td>\n",
       "      <td>1282.086304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125545</th>\n",
       "      <td>587634</td>\n",
       "      <td>3342.954102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125546 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id         loss\n",
       "0            4  1567.002808\n",
       "1            6  1642.509399\n",
       "2            9  9320.904297\n",
       "3           12  5998.083008\n",
       "4           15   882.344055\n",
       "5           17  1835.220581\n",
       "6           21  2105.648926\n",
       "7           28  1097.714600\n",
       "8           32  2825.964355\n",
       "9           43  3117.325439\n",
       "10          46  2651.223389\n",
       "11          50  1157.201416\n",
       "12          54  1314.928711\n",
       "13          62  2376.517822\n",
       "14          70  1742.467407\n",
       "15          71  5445.864746\n",
       "16          75  1979.991455\n",
       "17          77  2537.429688\n",
       "18          81  2123.841797\n",
       "19          83  2320.404053\n",
       "20          87  1807.947388\n",
       "21          97  1847.058105\n",
       "22         103  1144.983398\n",
       "23         119  1103.953491\n",
       "24         120  1414.260254\n",
       "25         127  1052.531860\n",
       "26         138  4996.552246\n",
       "27         141  2968.937744\n",
       "28         148   932.266968\n",
       "29         150  2150.988525\n",
       "...        ...          ...\n",
       "125516  587482  1385.335938\n",
       "125517  587484  4998.928223\n",
       "125518  587489  1797.644531\n",
       "125519  587494  1685.208862\n",
       "125520  587509  1082.122192\n",
       "125521  587511   913.553467\n",
       "125522  587515  1287.169556\n",
       "125523  587517  2376.877686\n",
       "125524  587519  1525.345947\n",
       "125525  587524   859.239380\n",
       "125526  587531  5675.145020\n",
       "125527  587532  6627.125000\n",
       "125528  587534  1983.960205\n",
       "125529  587538  2301.583252\n",
       "125530  587540  3059.468262\n",
       "125531  587548  1145.381592\n",
       "125532  587549  7495.793945\n",
       "125533  587560  3839.278320\n",
       "125534  587561  1589.709595\n",
       "125535  587581  1331.937256\n",
       "125536  587583  1663.251465\n",
       "125537  587587  1705.305908\n",
       "125538  587596  1809.606934\n",
       "125539  587610  1721.611816\n",
       "125540  587613  1630.627563\n",
       "125541  587617  2110.746338\n",
       "125542  587621  2785.067871\n",
       "125543  587627  2483.259766\n",
       "125544  587629  1282.086304\n",
       "125545  587634  3342.954102\n",
       "\n",
       "[125546 rows x 2 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output.to_csv(\"first_pass.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good for 1949 place out of 3000 teams. (1168.6)   Not great, but many of those team copy and pasted kernels published.  Good effort vs reward. as top output has a loss of 1197"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
